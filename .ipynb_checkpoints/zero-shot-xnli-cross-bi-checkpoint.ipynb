{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593da09b-829e-456e-a2d7-6849fedda263",
   "metadata": {},
   "source": [
    "# Zero-Shot Cross-Lingual Natural Language Inference with Bi-Encoders and Cross-Encoders Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d3926-2903-4ce3-8aa1-4c1639aec72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß± Step 1: Install Required Libraries\n",
    "# Ensure all required libraries are installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_libraries = [\"transformers\", \"datasets\", \"torch\", \"scikit-learn\"]\n",
    "for lib in required_libraries:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61b083-4898-4e68-86cf-4c7ac515d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Step 2: Import Required Libraries\n",
    "\n",
    "# Standard Library Imports\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d08f75-2ba2-4897-8a5e-4abea7423065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Step 3: Load Datasets (MNLI and XNLI)\n",
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load MNLI and XNLI datasets.\n",
    "    Returns:\n",
    "        mnli_dataset: MNLI dataset\n",
    "        xnli_es_dataset: XNLI Spanish dataset\n",
    "        xnli_de_dataset: XNLI German dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mnli_dataset = load_dataset(\"glue\", \"mnli\")\n",
    "        xnli_es_dataset = load_dataset(\"xnli\", \"es\")\n",
    "        xnli_de_dataset = load_dataset(\"xnli\", \"de\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "        raise\n",
    "    return mnli_dataset, xnli_es_dataset, xnli_de_dataset\n",
    "\n",
    "mnli_dataset, xnli_es_dataset, xnli_de_dataset = load_datasets()\n",
    "print(\"MNLI Dataset:\", mnli_dataset)\n",
    "print(\"XNLI Spanish Dataset:\", xnli_es_dataset)\n",
    "print(\"XNLI German Dataset:\", xnli_de_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508327c-6c36-476a-b816-574c66a90946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Step 4: Data Inspection and Class Distribution Analysis\n",
    "def analyze_class_distribution(dataset, split_name):\n",
    "    \"\"\"\n",
    "    Analyze and visualize class distribution in a dataset split.\n",
    "    Args:\n",
    "        dataset: The dataset to analyze.\n",
    "        split_name: The split name (e.g., \"train\", \"test\").\n",
    "    \"\"\"\n",
    "    labels = dataset[split_name][\"label\"]\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    label_percentages = {label: count / total_samples * 100 for label, count in label_counts.items()}\n",
    "    df = pd.DataFrame({\n",
    "        \"Label\": [\"Entailment\", \"Neutral\", \"Contradiction\"],\n",
    "        \"Count\": [label_counts.get(0, 0), label_counts.get(1, 0), label_counts.get(2, 0)],\n",
    "        \"Percentage\": [label_percentages.get(0, 0), label_percentages.get(1, 0), label_percentages.get(2, 0)]\n",
    "    })\n",
    "    print(f\"\\n--- Class Distribution of {split_name} ---\")\n",
    "    print(df.to_markdown(index=False))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=df[\"Label\"], y=df[\"Percentage\"])\n",
    "    plt.title(f\"Class Distribution of {split_name}\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyze class distribution for each dataset split\n",
    "analyze_class_distribution(mnli_dataset, \"train\")\n",
    "analyze_class_distribution(mnli_dataset, \"validation_matched\")\n",
    "analyze_class_distribution(xnli_es_dataset, \"test\")\n",
    "analyze_class_distribution(xnli_de_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00257939-b3b3-433d-a412-c955c89e06a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Step 5: Tokenization Check and Setting `max_length`\n",
    "def analyze_token_lengths(dataset, tokenizer, split_name):\n",
    "    \"\"\"\n",
    "    Analyze token lengths in a dataset split.\n",
    "    Args:\n",
    "        dataset: The dataset to analyze.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        split_name: The split name (e.g., \"train\").\n",
    "    Returns:\n",
    "        max_length: Recommended maximum token length.\n",
    "    \"\"\"\n",
    "    def get_token_lengths(examples):\n",
    "        return {\n",
    "            \"premise_length\": [len(tokenizer.encode(p)) for p in examples[\"premise\"]],\n",
    "            \"hypothesis_length\": [len(tokenizer.encode(h)) for h in examples[\"hypothesis\"]],\n",
    "        }\n",
    "\n",
    "    # Apply the function and remove only the original columns\n",
    "    token_lengths = dataset[split_name].map(\n",
    "        get_token_lengths,\n",
    "        batched=True,\n",
    "        remove_columns=[\"premise\", \"hypothesis\"]\n",
    "    )\n",
    "\n",
    "    # Extract the lengths from the dataset\n",
    "    premise_lengths = token_lengths[\"premise_length\"]\n",
    "    hypothesis_lengths = token_lengths[\"hypothesis_length\"]\n",
    "\n",
    "    all_lengths = np.array(premise_lengths + hypothesis_lengths)\n",
    "\n",
    "    print(f\"\\n--- Token Length Statistics ({split_name}) ---\")\n",
    "    df_lengths = pd.DataFrame({\n",
    "        \"Metric\": [\"Mean\", \"Median\", \"P90\", \"P95\", \"Max\"],\n",
    "        \"Premise Length\": [np.mean(premise_lengths), np.median(premise_lengths), np.percentile(premise_lengths, 90), np.percentile(premise_lengths, 95), np.max(premise_lengths)],\n",
    "        \"Hypothesis Length\": [np.mean(hypothesis_lengths), np.median(hypothesis_lengths), np.percentile(hypothesis_lengths, 90), np.percentile(hypothesis_lengths, 95), np.max(hypothesis_lengths)],\n",
    "        \"Combined Length\": [np.mean(all_lengths), np.median(all_lengths), np.percentile(all_lengths, 90), np.percentile(all_lengths, 95), np.max(all_lengths)],\n",
    "    })\n",
    "    print(df_lengths.to_markdown(index=False))\n",
    "\n",
    "    sns.histplot(all_lengths, bins=50)\n",
    "    plt.title(f\"Distribution of Combined Token Lengths ({split_name})\")\n",
    "    plt.xlabel(\"Token Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.axvline(128, color='r', linestyle='--', label='max_length = 128')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    max_length = 128  # Based on analysis\n",
    "    return max_length\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Tokenizer Loaded:\", tokenizer)\n",
    "\n",
    "max_length = analyze_token_lengths(mnli_dataset, tokenizer, \"train\")\n",
    "print(f\"Recommended max_length: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ef011-a14c-4564-8c51-213bd5997ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ Step 6: Tokenizer and Preprocessing Function\n",
    "def tokenize_batch(batch):\n",
    "    \"\"\"\n",
    "    Tokenizes premise and hypothesis pairs in a batch.\n",
    "    Args:\n",
    "        batch (dict): A batch containing 'premise' and 'hypothesis' fields.\n",
    "    Returns:\n",
    "        dict: Tokenized inputs with padding and truncation applied.\n",
    "    \"\"\"\n",
    "    return tokenizer(batch[\"premise\"], batch[\"hypothesis\"],\n",
    "                     padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "def rename_label(example):\n",
    "    \"\"\"\n",
    "    Renames the 'label' field to 'labels' for compatibility with PyTorch.\n",
    "    Args:\n",
    "        example (dict): A single example containing a 'label' field.\n",
    "    Returns:\n",
    "        dict: Example with the 'label' field renamed to 'labels'.\n",
    "    \"\"\"\n",
    "    return {\"labels\": example[\"label\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d37f6-d323-4be4-9822-7d751ff9e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Step 7: Apply Tokenization to Datasets\n",
    "def preprocess_and_format(dataset, split_name, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Tokenizes, renames labels, and formats a dataset split for PyTorch.\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset to preprocess.\n",
    "        split_name (str): The split name (e.g., \"train\", \"test\").\n",
    "        columns_to_remove (list): List of columns to remove after tokenization.\n",
    "    Returns:\n",
    "        Dataset: Preprocessed and formatted dataset.\n",
    "    \"\"\"\n",
    "    encoded_dataset = dataset[split_name].map(tokenize_batch, batched=True)\n",
    "    encoded_dataset = encoded_dataset.map(rename_label)\n",
    "    encoded_dataset = encoded_dataset.remove_columns(columns_to_remove)\n",
    "    encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])\n",
    "    return encoded_dataset\n",
    "\n",
    "mnli_encoded_train = preprocess_and_format(mnli_dataset, \"train\", [\"premise\", \"hypothesis\", \"label\", \"idx\"])\n",
    "print(\"MNLI Training set tokenized and formatted:\", mnli_encoded_train)\n",
    "\n",
    "mnli_encoded_validation = preprocess_and_format(mnli_dataset, \"validation_matched\", [\"premise\", \"hypothesis\", \"label\", \"idx\"])\n",
    "print(\"MNLI Validation set tokenized and formatted:\", mnli_encoded_validation)\n",
    "\n",
    "xnli_es_encoded = preprocess_and_format(xnli_es_dataset, \"test\", [\"premise\", \"hypothesis\", \"label\"])\n",
    "print(\"XNLI Spanish test set tokenized and formatted:\", xnli_es_encoded)\n",
    "\n",
    "xnli_de_encoded = preprocess_and_format(xnli_de_dataset, \"test\", [\"premise\", \"hypothesis\", \"label\"])\n",
    "print(\"XNLI German test set tokenized and formatted:\", xnli_de_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60a321-6d64-4c79-911d-fd273d1bf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÇÔ∏è Step 8: Create Baseline Training Subset\n",
    "def create_training_subset(dataset, test_size=0.9, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a smaller training subset for baseline experiments.\n",
    "    Args:\n",
    "        dataset (Dataset): The full training dataset.\n",
    "        test_size (float): Proportion of the dataset to use as the test set.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        Dataset: Training subset.\n",
    "    \"\"\"\n",
    "    return dataset.train_test_split(test_size=test_size, seed=seed)[\"train\"]\n",
    "\n",
    "train_subset = create_training_subset(mnli_encoded_train)\n",
    "print(\"Baseline training subset created:\", train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf99afc-f2fe-4621-b800-9317d76fba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 9: DataLoader Creation Utility\n",
    "def create_dataloader(encoded_dataset, batch_size=16, shuffle=False):\n",
    "    \"\"\"\n",
    "    Creates a PyTorch DataLoader for a given dataset.\n",
    "    Args:\n",
    "        encoded_dataset (Dataset): The dataset to load.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "    return DataLoader(encoded_dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113909b7-6c4f-455d-84a2-6900e26b5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Step 10: DataLoader Creation for Baseline\n",
    "baseline_train_dataloader = create_dataloader(train_subset, batch_size=64, shuffle=True)\n",
    "baseline_val_dataloader = create_dataloader(mnli_encoded_validation, batch_size=64) # Use the full validation set for evaluation\n",
    "print(\"Baseline training DataLoader created:\", baseline_train_dataloader)\n",
    "print(\"Baseline validation DataLoader created:\", baseline_val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81484f-f0d5-4d93-aa37-4231f28ef4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö°Ô∏è Step 11: Confirm GPU Availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"\\nUsing device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57276324-cead-4cc7-b09d-72bd55404e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Step 12: Define Pretrained Multilingual Cross-Encoder Model\n",
    "# Load a pretrained multilingual BERT model with a classification head for 3 labels (Entailment, Neutral, Contradiction)\n",
    "try:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-multilingual-cased\",\n",
    "        num_labels=3,\n",
    "    ).to(device)\n",
    "    print(\"Pretrained multilingual BERT model loaded with classification head and moved to device.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a776813-8431-4588-9403-f10ab4caa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Step 13: Define Evaluation Metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Computes evaluation metrics for model predictions.\n",
    "    Args:\n",
    "        eval_pred (tuple): A tuple containing logits and true labels.\n",
    "    Returns:\n",
    "        dict: A dictionary with accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "print(\"Evaluation metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce851dfb-a305-4527-b3c7-6e32a45bb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 14: Define Training Arguments for Baseline\n",
    "# Define key training parameters\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 1\n",
    "batch_size = 64\n",
    "\n",
    "baseline_training_args = TrainingArguments(\n",
    "    output_dir=\"./results-baseline-crossencoder\",  # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    save_total_limit=1,  # Keep only the most recent checkpoint\n",
    "    learning_rate=learning_rate,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=batch_size,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
    "    num_train_epochs=num_train_epochs,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    logging_dir=\"./logs-baseline\",  # Directory for logging\n",
    "    logging_strategy=\"epoch\",  # Log metrics at the end of each epoch\n",
    "    report_to=\"none\"  # Disable reporting to external services\n",
    ")\n",
    "print(\"Baseline training arguments defined:\", baseline_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f8f09-a736-4ae1-8aed-c7cbebb17c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üöÄ Step 15: Initialize and Train Baseline Cross-Encoder (Corrected)\n",
    "try:\n",
    "    baseline_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=baseline_training_args,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=mnli_encoded_validation,  # Use the encoded Dataset here\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"Baseline Trainer initialized:\", baseline_trainer)\n",
    "\n",
    "    baseline_results = baseline_trainer.train()\n",
    "    print(\"Baseline training completed:\", baseline_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8432324-5ed2-42c7-b9b5-3c646d0c1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Step 16: Evaluate Baseline on MNLI Validation and Print Metrics\n",
    "try:\n",
    "    baseline_eval_results = baseline_trainer.evaluate()\n",
    "\n",
    "    print(\"\\n--- Baseline Evaluation Results (MNLI Validation) ---\")\n",
    "    df_baseline_metrics = pd.DataFrame([baseline_eval_results])\n",
    "    print(df_baseline_metrics[[\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]].to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- Baseline Evaluation Dictionary ---\")\n",
    "    print(baseline_eval_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed1c14-9810-441f-84e6-8e0f7a33406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 17: Define Training Arguments for Full Model\n",
    "# Define key training parameters\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "full_model_training_args = TrainingArguments(\n",
    "    output_dir=\"./results-full-crossencoder\",  # Directory to save model checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    save_total_limit=1,  # Keep only the most recent checkpoint\n",
    "    learning_rate=learning_rate,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=batch_size,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,  # Batch size for evaluation\n",
    "    num_train_epochs=num_train_epochs,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    logging_dir=\"./logs-full\",  # Directory for logging\n",
    "    logging_strategy=\"epoch\",  # Log metrics at the end of each epoch\n",
    "    report_to=\"none\"  # Disable reporting to external services\n",
    ")\n",
    "print(\"Full model training arguments defined:\", full_model_training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda9b9d-f257-462a-96ab-2543f1561878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Step 18: Initialize and Train Full Cross-Encoder\n",
    "try:\n",
    "    full_model_trainer = Trainer(\n",
    "        model=model,  # We are using the same model instance, which will continue training\n",
    "        args=full_model_training_args,\n",
    "        train_dataset=mnli_encoded_train,  # Use the full encoded training set\n",
    "        eval_dataset=mnli_encoded_validation,  # Use the full encoded validation set\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"Full model Trainer initialized:\", full_model_trainer)\n",
    "\n",
    "    full_model_results = full_model_trainer.train()\n",
    "    print(\"Full model training completed:\", full_model_results)\n",
    "\n",
    "    # üß™ Evaluate Full Model on MNLI Validation Set\n",
    "    full_model_eval_results_mnli = full_model_trainer.evaluate()\n",
    "\n",
    "    print(\"\\n--- Full Model Evaluation Results (MNLI Validation) ---\")\n",
    "    df_full_model_metrics_mnli = pd.DataFrame([full_model_eval_results_mnli])\n",
    "    print(df_full_model_metrics_mnli[[\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]].to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- Full Model Evaluation Dictionary (MNLI Validation) ---\")\n",
    "    print(full_model_eval_results_mnli)\n",
    "except Exception as e:\n",
    "    print(f\"Error during training or evaluation: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce3518-65fd-4cc2-80cc-f386d458dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåç Step 19: Evaluate Full Model on XNLI Spanish Test Set\n",
    "try:\n",
    "    full_model_eval_results_xnli_es = full_model_trainer.evaluate(xnli_es_encoded, metric_key_prefix=\"xnli_es\")\n",
    "\n",
    "    print(\"\\n--- Full Model Evaluation Results (XNLI Spanish Test) ---\")\n",
    "    df_full_model_metrics_xnli_es = pd.DataFrame([full_model_eval_results_xnli_es])\n",
    "    print(df_full_model_metrics_xnli_es[[\"xnli_es_accuracy\", \"xnli_es_precision\", \"xnli_es_recall\", \"xnli_es_f1\"]].to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- Full Model Evaluation Dictionary (XNLI Spanish Test) ---\")\n",
    "    print(full_model_eval_results_xnli_es)\n",
    "\n",
    "    # Save Cross-Encoder Metrics for XNLI Spanish\n",
    "    cross_encoder_xnli_es_metrics = {\n",
    "        \"accuracy\": full_model_eval_results_xnli_es[\"xnli_es_accuracy\"],\n",
    "        \"precision\": full_model_eval_results_xnli_es[\"xnli_es_precision\"],\n",
    "        \"recall\": full_model_eval_results_xnli_es[\"xnli_es_recall\"],\n",
    "        \"f1\": full_model_eval_results_xnli_es[\"xnli_es_f1\"],\n",
    "    }\n",
    "    print(\"\\n--- Saved Cross-Encoder Metrics (XNLI Spanish) ---\")\n",
    "    print(cross_encoder_xnli_es_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation on XNLI Spanish Test Set: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57250512-140e-4a6f-bc96-394797d58481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üá©üá™ Step 20: Evaluate Full Model on XNLI German Test Set\n",
    "try:\n",
    "    full_model_eval_results_xnli_de = full_model_trainer.evaluate(xnli_de_encoded, metric_key_prefix=\"xnli_de\")\n",
    "\n",
    "    print(\"\\n--- Full Model Evaluation Results (XNLI German Test) ---\")\n",
    "    df_full_model_metrics_xnli_de = pd.DataFrame([full_model_eval_results_xnli_de])\n",
    "    print(df_full_model_metrics_xnli_de[[\"xnli_de_accuracy\", \"xnli_de_precision\", \"xnli_de_recall\", \"xnli_de_f1\"]].to_markdown(index=False))\n",
    "\n",
    "    print(\"\\n--- Full Model Evaluation Dictionary (XNLI German Test) ---\")\n",
    "    print(full_model_eval_results_xnli_de)\n",
    "\n",
    "    # Save Cross-Encoder Metrics for XNLI German\n",
    "    cross_encoder_xnli_de_metrics = {\n",
    "        \"accuracy\": full_model_eval_results_xnli_de[\"xnli_de_accuracy\"],\n",
    "        \"precision\": full_model_eval_results_xnli_de[\"xnli_de_precision\"],\n",
    "        \"recall\": full_model_eval_results_xnli_de[\"xnli_de_recall\"],\n",
    "        \"f1\": full_model_eval_results_xnli_de[\"xnli_de_f1\"],\n",
    "    }\n",
    "    print(\"\\n--- Saved Cross-Encoder Metrics (XNLI German) ---\")\n",
    "    print(cross_encoder_xnli_de_metrics)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation on XNLI German Test Set: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e68ed-274c-44c4-a6b1-5730f51754f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Step 21: Initialize the Bi-Encoder Model\n",
    "class BiEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Bi-Encoder model for generating embeddings for premise and hypothesis pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        super(BiEncoder, self).__init__()\n",
    "        try:\n",
    "            self.transformer = AutoModel.from_pretrained(model_name)\n",
    "            self.embedding_dim = self.transformer.config.hidden_size\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Bi-Encoder: {e}\")\n",
    "            raise\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for the Bi-Encoder.\n",
    "        Args:\n",
    "            input_ids: Tokenized input IDs.\n",
    "            attention_mask: Attention mask for the input.\n",
    "            token_type_ids: Token type IDs for the input.\n",
    "        Returns:\n",
    "            cls_embedding: Embedding of the [CLS] token.\n",
    "        \"\"\"\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "        cls_embedding = outputs.pooler_output  # Extract the [CLS] token embedding\n",
    "        return cls_embedding\n",
    "\n",
    "bi_encoder_model_name = \"bert-base-multilingual-cased\"\n",
    "bi_encoder = BiEncoder(bi_encoder_model_name)\n",
    "print(\"Bi-Encoder Model Initialized:\", bi_encoder)\n",
    "print(\"Bi-Encoder Embedding Dimension:\", bi_encoder.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a8471-8f52-41ea-979d-a31ea3d2fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 22: Load and Encode MNLI Datasets for Bi-Encoder (Robust)\n",
    "\n",
    "# Load the MNLI dataset\n",
    "mnli_dataset = load_dataset(\"multi_nli\")\n",
    "mnli_train_dataset = mnli_dataset[\"train\"]\n",
    "mnli_val_dataset = mnli_dataset[\"validation_matched\"]  # Using \"matched\" for validation\n",
    "\n",
    "# Tokenization function for bi-encoder (separate premise and hypothesis)\n",
    "def encode_bi_encoder_robust(examples):\n",
    "    premises = examples[\"premise\"]\n",
    "    hypotheses = examples[\"hypothesis\"]\n",
    "\n",
    "    encoded_premises = tokenizer(premises, truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "    encoded_hypotheses = tokenizer(hypotheses, truncation=True, padding=\"max_length\", max_length=100, return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"premise_input_ids\": encoded_premises[\"input_ids\"],\n",
    "        \"premise_attention_mask\": encoded_premises[\"attention_mask\"],\n",
    "        \"premise_token_type_ids\": encoded_premises[\"token_type_ids\"],\n",
    "        \"hypothesis_input_ids\": encoded_hypotheses[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": encoded_hypotheses[\"attention_mask\"],\n",
    "        \"hypothesis_token_type_ids\": encoded_hypotheses[\"token_type_ids\"],\n",
    "        \"labels\": examples[\"label\"]\n",
    "    }\n",
    "\n",
    "# Encode the MNLI training and validation datasets\n",
    "encoded_mnli_train = mnli_train_dataset.map(encode_bi_encoder_robust, batched=True)\n",
    "encoded_mnli_val = mnli_val_dataset.map(encode_bi_encoder_robust, batched=True)\n",
    "\n",
    "# Set the format for PyTorch tensors and specify the columns to return\n",
    "encoded_mnli_train.set_format(\"torch\", columns=[\n",
    "    \"premise_input_ids\", \"premise_attention_mask\", \"premise_token_type_ids\",\n",
    "    \"hypothesis_input_ids\", \"hypothesis_attention_mask\", \"hypothesis_token_type_ids\", \"labels\"\n",
    "])\n",
    "encoded_mnli_val.set_format(\"torch\", columns=[\n",
    "    \"premise_input_ids\", \"premise_attention_mask\", \"premise_token_type_ids\",\n",
    "    \"hypothesis_input_ids\", \"hypothesis_attention_mask\", \"hypothesis_token_type_ids\", \"labels\"\n",
    "])\n",
    "\n",
    "print(\"Encoded MNLI Training Dataset (Robust):\", encoded_mnli_train)\n",
    "print(\"Encoded MNLI Validation Dataset (Robust):\", encoded_mnli_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4341c9f-29a1-49bd-9ded-2e80842aebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 23: Create DataLoaders for Bi-Encoder Training\n",
    "# Create DataLoaders for training and validation datasets\n",
    "# Shuffle the training DataLoader to ensure randomness in batches\n",
    "bi_encoder_train_dataloader = DataLoader(encoded_mnli_train, batch_size=32, shuffle=True)\n",
    "bi_encoder_val_dataloader = DataLoader(encoded_mnli_val, batch_size=32)\n",
    "\n",
    "print(\"Bi-Encoder Training DataLoader created with batch size 32.\")\n",
    "print(\"Bi-Encoder Validation DataLoader created with batch size 32.\")\n",
    "\n",
    "# Uncomment the following lines to inspect a batch\n",
    "# for batch in bi_encoder_train_dataloader:\n",
    "#     print(\"Batch keys:\", batch.keys())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70148834-7fc2-47bc-bc34-7e25875255b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Step 24: Initialize the Bi-Encoder Classification Head\n",
    "class BiEncoderClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification head for the Bi-Encoder model.\n",
    "    Combines premise and hypothesis embeddings and predicts the label.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super(BiEncoderClassificationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_dim * 3, embedding_dim)  # Concatenate u, v, |u-v|\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, premise_embedding, hypothesis_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass for the classification head.\n",
    "        Args:\n",
    "            premise_embedding: Embedding of the premise.\n",
    "            hypothesis_embedding: Embedding of the hypothesis.\n",
    "        Returns:\n",
    "            output: Logits for each class.\n",
    "        \"\"\"\n",
    "        # Calculate the absolute difference between the embeddings\n",
    "        abs_diff = torch.abs(premise_embedding - hypothesis_embedding)\n",
    "\n",
    "        # Concatenate the premise embedding, hypothesis embedding, and their absolute difference\n",
    "        concatenated_embedding = torch.cat((premise_embedding, hypothesis_embedding, abs_diff), dim=-1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        output = self.fc1(concatenated_embedding)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "# Initialize the classification head\n",
    "num_nli_labels = 3  # Entailment, Neutral, Contradiction\n",
    "bi_encoder_classifier = BiEncoderClassificationHead(bi_encoder.embedding_dim, num_nli_labels)\n",
    "print(\"Bi-Encoder Classification Head Initialized:\", bi_encoder_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac085c-52e1-412d-b11c-daf8dbc120e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 25: Define Loss Function and Optimizer\n",
    "# Cross-entropy loss is used for multi-class classification (Entailment, Neutral, Contradiction)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# AdamW optimizer is used for training transformer-based models with weight decay\n",
    "optimizer = optim.AdamW(\n",
    "    list(bi_encoder.parameters()) + list(bi_encoder_classifier.parameters()), \n",
    "    lr=2e-5  # Learning rate\n",
    ")\n",
    "\n",
    "print(\"Loss Function:\", criterion)\n",
    "print(\"Optimizer:\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f71117-e4ce-4d13-80a2-bb7a3340eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 26: Define train_bi_encoder_step Function\n",
    "def train_bi_encoder_step(model, classifier, optimizer, criterion, batch, device):\n",
    "    \"\"\"\n",
    "    Performs a single training step for the Bi-Encoder model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Bi-Encoder model.\n",
    "        classifier (nn.Module): The classification head for the Bi-Encoder.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        criterion (nn.Module): Loss function for calculating the training loss.\n",
    "        batch (dict): A batch of input data containing premise, hypothesis, and labels.\n",
    "        device (torch.device): The device (CPU or GPU) to run the training step on.\n",
    "\n",
    "    Returns:\n",
    "        float: The loss value for the current training step.\n",
    "    \"\"\"\n",
    "    # Set the model and classifier to training mode\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Move batch data to the specified device\n",
    "    premise_input_ids = batch[\"premise_input_ids\"].to(device)\n",
    "    premise_attention_mask = batch[\"premise_attention_mask\"].to(device)\n",
    "    premise_token_type_ids = batch[\"premise_token_type_ids\"].to(device)\n",
    "    hypothesis_input_ids = batch[\"hypothesis_input_ids\"].to(device)\n",
    "    hypothesis_attention_mask = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "    hypothesis_token_type_ids = batch[\"hypothesis_token_type_ids\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    # Get embeddings from the Bi-Encoder for premise and hypothesis\n",
    "    premise_embedding = model(premise_input_ids, premise_attention_mask, premise_token_type_ids)\n",
    "    hypothesis_embedding = model(hypothesis_input_ids, hypothesis_attention_mask, hypothesis_token_type_ids)\n",
    "\n",
    "    # Pass the embeddings through the classification head to get logits\n",
    "    logits = classifier(premise_embedding, hypothesis_embedding)\n",
    "\n",
    "    # Calculate the loss using the criterion\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # Backpropagate the loss and update model weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return the loss value for logging\n",
    "    return loss.item()\n",
    "\n",
    "print(\"Function 'train_bi_encoder_step' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f398ad-18b5-456e-afa5-1ec2ddc050ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 27: Define `evaluate_bi_encoder` Function\n",
    "def evaluate_bi_encoder(model, classifier, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the Bi-Encoder model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Bi-Encoder model.\n",
    "        classifier (nn.Module): The classification head for the Bi-Encoder.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        criterion (nn.Module): Loss function for calculating the evaluation loss.\n",
    "        device (torch.device): The device (CPU or GPU) to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - avg_loss (float): Average loss over the dataset.\n",
    "            - accuracy (float): Accuracy of the model.\n",
    "            - precision (float): Macro-averaged precision.\n",
    "            - recall (float): Macro-averaged recall.\n",
    "            - f1 (float): Macro-averaged F1 score.\n",
    "            - per_class_precision (list): Precision for each class.\n",
    "            - per_class_recall (list): Recall for each class.\n",
    "            - per_class_f1 (list): F1 score for each class.\n",
    "            - cm (ndarray): Confusion matrix.\n",
    "            - predictions (ndarray): Model predictions.\n",
    "            - labels (ndarray): True labels.\n",
    "    \"\"\"\n",
    "    # Set the model and classifier to evaluation mode\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    # Initialize metrics\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_samples = 0\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch data to the specified device\n",
    "            premise_input_ids = batch[\"premise_input_ids\"].to(device)\n",
    "            premise_attention_mask = batch[\"premise_attention_mask\"].to(device)\n",
    "            premise_token_type_ids = batch[\"premise_token_type_ids\"].to(device)\n",
    "            hypothesis_input_ids = batch[\"hypothesis_input_ids\"].to(device)\n",
    "            hypothesis_attention_mask = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "            hypothesis_token_type_ids = batch[\"hypothesis_token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Get embeddings from the Bi-Encoder\n",
    "            premise_embedding = model(premise_input_ids, premise_attention_mask, premise_token_type_ids)\n",
    "            hypothesis_embedding = model(hypothesis_input_ids, hypothesis_attention_mask, hypothesis_token_type_ids)\n",
    "\n",
    "            # Pass embeddings through the classification head\n",
    "            logits = classifier(premise_embedding, hypothesis_embedding)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * premise_input_ids.size(0)\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    per_class_precision, per_class_recall, per_class_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average=None, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Return metrics and predictions\n",
    "    return avg_loss, accuracy, precision, recall, f1, per_class_precision, per_class_recall, per_class_f1, cm, np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "print(\"Enhanced function 'evaluate_bi_encoder' defined (with predictions).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919520c1-a63e-47a0-b4a4-08ee5270394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 28: Implement the Full Bi-Encoder Training Loop\n",
    "def train_bi_encoder(\n",
    "    bi_encoder, bi_encoder_classifier, train_dataloader, val_dataloader, num_epochs, learning_rate, device, output_log_file\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the Bi-Encoder model with a classification head.\n",
    "\n",
    "    Args:\n",
    "        bi_encoder (nn.Module): The Bi-Encoder model.\n",
    "        bi_encoder_classifier (nn.Module): The classification head for the Bi-Encoder.\n",
    "        train_dataloader (DataLoader): DataLoader for the training dataset.\n",
    "        val_dataloader (DataLoader): DataLoader for the validation dataset.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        device (torch.device): The device (CPU or GPU) to run the training on.\n",
    "        output_log_file (str): Path to the log file for recording training progress.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Move models to the device\n",
    "    bi_encoder.to(device)\n",
    "    bi_encoder_classifier.to(device)\n",
    "\n",
    "    # Define optimizer and scheduler\n",
    "    optimizer = AdamW(list(bi_encoder.parameters()) + list(bi_encoder_classifier.parameters()), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"Training on: {device}\")\n",
    "\n",
    "    # Open log file for writing\n",
    "    with open(output_log_file, \"w\") as f:\n",
    "        f.write(\"Epoch,Step,Loss,Time\\n\")\n",
    "        f.write(\"Epoch,Validation Loss,Validation Accuracy,Validation Precision (Macro),Validation Recall (Macro),Validation F1 (Macro)\\n\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_total_loss = 0\n",
    "        num_steps = 0\n",
    "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i, batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move batch data to the specified device\n",
    "            premise_input_ids = batch[\"premise_input_ids\"].to(device)\n",
    "            premise_attention_mask = batch[\"premise_attention_mask\"].to(device)\n",
    "            premise_token_type_ids = batch[\"premise_token_type_ids\"].to(device)\n",
    "            hypothesis_input_ids = batch[\"hypothesis_input_ids\"].to(device)\n",
    "            hypothesis_attention_mask = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "            hypothesis_token_type_ids = batch[\"hypothesis_token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            premise_embedding = bi_encoder(premise_input_ids, premise_attention_mask, premise_token_type_ids)\n",
    "            hypothesis_embedding = bi_encoder(hypothesis_input_ids, hypothesis_attention_mask, hypothesis_token_type_ids)\n",
    "            logits = bi_encoder_classifier(premise_embedding, hypothesis_embedding)\n",
    "\n",
    "            # Compute loss and backpropagate\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_total_loss += loss.item()\n",
    "            num_steps += 1\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss:.4f}\"})\n",
    "\n",
    "        # Evaluate on the validation set at the end of each epoch\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1, _, _, _, _ = evaluate_bi_encoder(\n",
    "            bi_encoder, bi_encoder_classifier, val_dataloader, criterion, device\n",
    "        )\n",
    "        avg_epoch_loss = epoch_total_loss / num_steps\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} Summary ---\")\n",
    "        print(f\"Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation Precision (Macro): {val_precision:.4f}\")\n",
    "        print(f\"Validation Recall (Macro): {val_recall:.4f}\")\n",
    "        print(f\"Validation F1 (Macro): {val_f1:.4f}\")\n",
    "        print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # Log epoch summary to file\n",
    "        with open(output_log_file, \"a\") as f:\n",
    "            f.write(f\"{epoch+1},{val_loss:.4f},{val_accuracy:.4f},{val_precision:.4f},{val_recall:.4f},{val_f1:.4f}\\n\")\n",
    "\n",
    "    print(\"\\nBi-Encoder Training Completed! Training details logged to:\", output_log_file)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_bi_encoder(\n",
    "    bi_encoder=bi_encoder,\n",
    "    bi_encoder_classifier=bi_encoder_classifier,\n",
    "    train_dataloader=bi_encoder_train_dataloader,\n",
    "    val_dataloader=bi_encoder_val_dataloader,\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    device=device,\n",
    "    output_log_file=\"bi_encoder_training_log.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd0dc4-097a-4868-83d5-c81a459069cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 29: Evaluate Bi-Encoder on MNLI Validation Set\n",
    "def evaluate_and_print_results(model, classifier, dataloader, criterion, device, dataset_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Evaluates the Bi-Encoder model and prints the results.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Bi-Encoder model.\n",
    "        classifier (nn.Module): The classification head for the Bi-Encoder.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        criterion (nn.Module): Loss function for calculating the evaluation loss.\n",
    "        device (torch.device): The device (CPU or GPU) to run the evaluation on.\n",
    "        dataset_name (str): Name of the dataset being evaluated.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Evaluate the model\n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1, val_pc_precision, val_pc_recall, val_pc_f1, val_cm, _, _ = evaluate_bi_encoder(\n",
    "        model, classifier, dataloader, criterion, device\n",
    "    )\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"\\n--- Bi-Encoder Evaluation Results ({dataset_name}) ---\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Macro-averaged Precision: {val_precision:.4f}\")\n",
    "    print(f\"Macro-averaged Recall: {val_recall:.4f}\")\n",
    "    print(f\"Macro-averaged F1-Score: {val_f1:.4f}\")\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, label in enumerate([\"Entailment\", \"Neutral\", \"Contradiction\"]):\n",
    "        print(f\"  {label}: Precision={val_pc_precision[i]:.4f}, Recall={val_pc_recall[i]:.4f}, F1-Score={val_pc_f1[i]:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(val_cm)\n",
    "\n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        \"loss\": val_loss,\n",
    "        \"accuracy\": val_accuracy,\n",
    "        \"precision\": val_precision,\n",
    "        \"recall\": val_recall,\n",
    "        \"f1\": val_f1,\n",
    "        \"per_class_precision\": val_pc_precision,\n",
    "        \"per_class_recall\": val_pc_recall,\n",
    "        \"per_class_f1\": val_pc_f1,\n",
    "        \"confusion_matrix\": val_cm,\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate on MNLI Validation Set\n",
    "mnli_validation_results = evaluate_and_print_results(\n",
    "    bi_encoder, bi_encoder_classifier, bi_encoder_val_dataloader, criterion, device, dataset_name=\"MNLI Validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c7961-381e-4620-a0be-c15f253455be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 30: Access Spanish XNLI Test Set\n",
    "def access_test_set(dataset, language=\"Spanish\"):\n",
    "    \"\"\"\n",
    "    Accesses the test set for a given language.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset containing the test set.\n",
    "        language (str): The language of the test set.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The test set for the specified language.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test_set = dataset[\"test\"]\n",
    "        print(f\"{language} XNLI Test Set accessed with {len(test_set)} samples.\")\n",
    "        print(test_set[0])  # Print the first sample for inspection\n",
    "        return test_set\n",
    "    except KeyError as e:\n",
    "        print(f\"Error accessing {language} XNLI Test Set: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Access Spanish XNLI Test Set\n",
    "xnli_es_test = access_test_set(xnli_es_dataset, language=\"Spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b8e9b-5752-4871-96e3-959df646a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 31: Access German XNLI Test Set\n",
    "def access_test_set(dataset, language=\"German\"):\n",
    "    \"\"\"\n",
    "    Accesses the test set for a given language.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset containing the test set.\n",
    "        language (str): The language of the test set.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The test set for the specified language.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test_set = dataset[\"test\"]\n",
    "        print(f\"{language} XNLI Test Set accessed with {len(test_set)} samples.\")\n",
    "        print(test_set[0])  # Print the first sample for inspection\n",
    "        return test_set\n",
    "    except KeyError as e:\n",
    "        print(f\"Error accessing {language} XNLI Test Set: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Access German XNLI Test Set\n",
    "xnli_de_test = access_test_set(xnli_de_dataset, language=\"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dd94c-65c7-4e0b-8d6a-1c4885088262",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check first element of Spanish XNLI test set\n",
    "# print(xnli_es_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3445e7a-abef-4261-ac0a-0d93e77f779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 32: Preprocess XNLI Test Set for Bi-Encoder\n",
    "def preprocess_xnli_bi_encoder(examples, premise_key=\"premise\", hypothesis_key=\"hypothesis\"):\n",
    "    \"\"\"\n",
    "    Preprocesses XNLI test set for the Bi-Encoder.\n",
    "\n",
    "    Args:\n",
    "        examples (dict): A batch of examples containing premise and hypothesis.\n",
    "        premise_key (str): Key for the premise field in the dataset.\n",
    "        hypothesis_key (str): Key for the hypothesis field in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Preprocessed and tokenized inputs for the Bi-Encoder.\n",
    "    \"\"\"\n",
    "    premise = examples[premise_key]\n",
    "    hypothesis = examples[hypothesis_key]\n",
    "    encoded_premise = tokenizer(premise, truncation=True, padding=\"max_length\", max_length=100)\n",
    "    encoded_hypothesis = tokenizer(hypothesis, truncation=True, padding=\"max_length\", max_length=100)\n",
    "    return {\n",
    "        \"premise_input_ids\": encoded_premise[\"input_ids\"],\n",
    "        \"premise_attention_mask\": encoded_premise[\"attention_mask\"],\n",
    "        \"premise_token_type_ids\": encoded_premise.get(\"token_type_ids\", [[0] * len(ids) for ids in encoded_premise[\"input_ids\"]]),\n",
    "        \"hypothesis_input_ids\": encoded_hypothesis[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": encoded_hypothesis[\"attention_mask\"],\n",
    "        \"hypothesis_token_type_ids\": encoded_hypothesis.get(\"token_type_ids\", [[0] * len(ids) for ids in encoded_hypothesis[\"input_ids\"]]),\n",
    "        \"labels\": examples[\"label\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Preprocess Spanish XNLI Test Set\n",
    "xnli_es_encoded_test = xnli_es_test.map(preprocess_xnli_bi_encoder, batched=True)\n",
    "xnli_es_encoded_test.set_format(\"torch\", columns=[\"premise_input_ids\", \"premise_attention_mask\", \"premise_token_type_ids\", \"hypothesis_input_ids\", \"hypothesis_attention_mask\", \"hypothesis_token_type_ids\", \"labels\"])\n",
    "\n",
    "print(\"Spanish XNLI Test Set preprocessed and formatted for Bi-Encoder.\")\n",
    "print(xnli_es_encoded_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b197a61-920a-4ff2-9e0a-bc6e01fc0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 33: Preprocess German XNLI Test Set for Bi-Encoder\n",
    "# Reuse the `preprocess_xnli_bi_encoder` function defined earlier\n",
    "xnli_de_encoded_test = xnli_de_test.map(preprocess_xnli_bi_encoder, batched=True)\n",
    "\n",
    "# Set the format for PyTorch tensors and specify the columns to return\n",
    "xnli_de_encoded_test.set_format(\n",
    "    \"torch\",\n",
    "    columns=[\n",
    "        \"premise_input_ids\",\n",
    "        \"premise_attention_mask\",\n",
    "        \"premise_token_type_ids\",\n",
    "        \"hypothesis_input_ids\",\n",
    "        \"hypothesis_attention_mask\",\n",
    "        \"hypothesis_token_type_ids\",\n",
    "        \"labels\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print confirmation and inspect the first preprocessed sample\n",
    "print(\"German XNLI Test Set preprocessed and formatted for Bi-Encoder.\")\n",
    "print(xnli_de_encoded_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcd227-2725-4887-a335-4a8e96cd592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 34: Create DataLoaders for Spanish and German XNLI Test Sets\n",
    "# Define evaluation batch size\n",
    "eval_batch_size = 32  # Adjust based on available memory and hardware\n",
    "\n",
    "# Create DataLoaders for Spanish and German XNLI Test Sets\n",
    "bi_encoder_es_test_dataloader = DataLoader(\n",
    "    xnli_es_encoded_test, batch_size=eval_batch_size, shuffle=False\n",
    ")\n",
    "bi_encoder_de_test_dataloader = DataLoader(\n",
    "    xnli_de_encoded_test, batch_size=eval_batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Print confirmation of DataLoader creation\n",
    "print(f\"DataLoader for Spanish XNLI Test Set created with {len(bi_encoder_es_test_dataloader)} batches.\")\n",
    "print(f\"DataLoader for German XNLI Test Set created with {len(bi_encoder_de_test_dataloader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b39ab-159c-434d-8143-22ac79510ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 35: Evaluate Bi-Encoder on Spanish and German XNLI Test Sets\n",
    "def print_evaluation_results(language, loss, accuracy, precision, recall, f1, per_class_precision, per_class_recall, per_class_f1, cm):\n",
    "    \"\"\"\n",
    "    Prints evaluation results for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        language (str): The language of the dataset (e.g., \"Spanish\", \"German\").\n",
    "        loss (float): Test loss.\n",
    "        accuracy (float): Test accuracy.\n",
    "        precision (float): Macro-averaged precision.\n",
    "        recall (float): Macro-averaged recall.\n",
    "        f1 (float): Macro-averaged F1 score.\n",
    "        per_class_precision (list): Precision for each class.\n",
    "        per_class_recall (list): Recall for each class.\n",
    "        per_class_f1 (list): F1 score for each class.\n",
    "        cm (ndarray): Confusion matrix.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Bi-Encoder Evaluation Results ({language} XNLI Test Set) ---\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro-averaged Precision: {precision:.4f}\")\n",
    "    print(f\"Macro-averaged Recall: {recall:.4f}\")\n",
    "    print(f\"Macro-averaged F1-Score: {f1:.4f}\")\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, label in enumerate([\"Entailment\", \"Neutral\", \"Contradiction\"]):\n",
    "        print(f\"  {label}: Precision={per_class_precision[i]:.4f}, Recall={per_class_recall[i]:.4f}, F1-Score={per_class_f1[i]:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Evaluate Bi-Encoder on Spanish XNLI Test Set\n",
    "es_test_loss, es_test_accuracy, es_precision, es_recall, es_f1, es_pc_precision, es_pc_recall, es_pc_f1, es_cm, es_bi_encoder_predictions, es_bi_encoder_labels = evaluate_bi_encoder(\n",
    "    bi_encoder, bi_encoder_classifier, bi_encoder_es_test_dataloader, criterion, device\n",
    ")\n",
    "print_evaluation_results(\"Spanish\", es_test_loss, es_test_accuracy, es_precision, es_recall, es_f1, es_pc_precision, es_pc_recall, es_pc_f1, es_cm)\n",
    "\n",
    "# Evaluate Bi-Encoder on German XNLI Test Set\n",
    "de_test_loss, de_test_accuracy, de_precision, de_recall, de_f1, de_pc_precision, de_pc_recall, de_pc_f1, de_cm, de_bi_encoder_predictions, de_bi_encoder_labels = evaluate_bi_encoder(\n",
    "    bi_encoder, bi_encoder_classifier, bi_encoder_de_test_dataloader, criterion, device\n",
    ")\n",
    "print_evaluation_results(\"German\", de_test_loss, de_test_accuracy, de_precision, de_recall, de_f1, de_pc_precision, de_pc_recall, de_pc_f1, de_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a33010-00e3-4ad9-8f67-f1deb2fd1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 36: Re-evaluate Bi-Encoder on MNLI Validation with Enhanced Metrics\n",
    "print(\"\\n--- Re-evaluating Bi-Encoder on MNLI Validation (Enhanced Metrics) ---\")\n",
    "\n",
    "# Perform evaluation on the MNLI validation set\n",
    "val_loss, val_accuracy, val_precision, val_recall, val_f1, val_pc_precision, val_pc_recall, val_pc_f1, val_cm, val_bi_encoder_predictions_again, val_bi_encoder_labels_again = evaluate_bi_encoder(\n",
    "    bi_encoder, bi_encoder_classifier, bi_encoder_val_dataloader, criterion, device\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print_evaluation_results(\n",
    "    \"MNLI Validation\",\n",
    "    val_loss,\n",
    "    val_accuracy,\n",
    "    val_precision,\n",
    "    val_recall,\n",
    "    val_f1,\n",
    "    val_pc_precision,\n",
    "    val_pc_recall,\n",
    "    val_pc_f1,\n",
    "    val_cm,\n",
    ")\n",
    "\n",
    "# Print shapes of predictions and labels for verification\n",
    "print(f\"Bi-Encoder Validation Predictions Shape (Re-evaluation): {val_bi_encoder_predictions_again.shape}\")\n",
    "print(f\"Bi-Encoder Validation True Labels Shape (Re-evaluation): {val_bi_encoder_labels_again.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fc7d7-804f-45aa-b654-e2a0bd7d673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 37: Retrain Cross-Encoder on Full MNLI Dataset\n",
    "print(\"\\n--- Retraining Cross-Encoder on Full MNLI Dataset ---\")\n",
    "\n",
    "# Re-initialize the cross-encoder model\n",
    "cross_encoder_retrained = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", num_labels=3\n",
    ").to(device)\n",
    "\n",
    "# Define training arguments for retraining\n",
    "training_args_retrained = TrainingArguments(\n",
    "    output_dir=\"./cross_encoder_retrained\",  # Directory to save the retrained model\n",
    "    num_train_epochs=3,                      # Number of training epochs (matching bi-encoder)\n",
    "    per_device_train_batch_size=64,          # Batch size for training\n",
    "    gradient_accumulation_steps=1,           # Gradient accumulation steps\n",
    "    learning_rate=2e-5,                      # Learning rate\n",
    "    weight_decay=0.01,                       # Weight decay for regularization\n",
    "    logging_dir=\"./logs_cross_encoder_retrained\",  # Directory for logging\n",
    "    logging_steps=500,                       # Log every 500 steps\n",
    "    evaluation_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                  # Save model at the end of each epoch\n",
    "    load_best_model_at_end=False,           # Do not load the best model automatically\n",
    "    metric_for_best_model=\"eval_loss\",      # Metric to determine the best model\n",
    "    report_to=\"none\"                        # Disable reporting to external services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for retraining\n",
    "trainer_retrained = Trainer(\n",
    "    model=cross_encoder_retrained,\n",
    "    args=training_args_retrained,\n",
    "    train_dataset=mnli_encoded_train,       # Full MNLI training dataset\n",
    "    eval_dataset=mnli_encoded_validation,  # MNLI validation dataset\n",
    "    tokenizer=tokenizer,                   # Tokenizer for preprocessing\n",
    "    compute_metrics=compute_metrics,       # Evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the cross-encoder model\n",
    "trainer_retrained.train()\n",
    "\n",
    "# Save the retrained model\n",
    "trainer_retrained.save_model(\"./cross_encoder_retrained_final\")\n",
    "print(\"--- Cross-Encoder Retraining Complete and Model Saved ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9faf7-b775-4d04-b1be-2fdf2734c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 38: Evaluate Retrained Cross-Encoder on MNLI and XNLI Test Sets\n",
    "def evaluate_cross_encoder(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the Cross-Encoder model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Cross-Encoder model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
    "        criterion (nn.Module): Loss function for calculating the evaluation loss.\n",
    "        device (torch.device): The device (CPU or GPU) to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing evaluation metrics and predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch data to the specified device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            token_type_ids = batch.get(\"token_type_ids\", None)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    per_class_precision, per_class_recall, per_class_f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average=None, zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, per_class_precision, per_class_recall, per_class_f1, cm, np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "# Evaluate on MNLI Validation Set\n",
    "print(\"\\n--- Evaluating Retrained Cross-Encoder on MNLI Validation ---\")\n",
    "ce_val_loss, ce_val_accuracy, ce_val_precision, ce_val_recall, ce_val_f1, ce_val_pc_precision, ce_val_pc_recall, ce_val_pc_f1, ce_val_cm, ce_val_predictions, ce_val_labels = evaluate_cross_encoder(\n",
    "    cross_encoder_retrained, cross_encoder_val_dataloader, cross_encoder_criterion, device\n",
    ")\n",
    "print_evaluation_results(\"MNLI Validation (Cross-Encoder)\", ce_val_loss, ce_val_accuracy, ce_val_precision, ce_val_recall, ce_val_f1, ce_val_pc_precision, ce_val_pc_recall, ce_val_pc_f1, ce_val_cm)\n",
    "\n",
    "# Evaluate on Spanish XNLI Test Set\n",
    "print(\"\\n--- Evaluating Retrained Cross-Encoder on Spanish XNLI Test Set ---\")\n",
    "ce_es_test_loss, ce_es_test_accuracy, ce_es_test_precision, ce_es_test_recall, ce_es_test_f1, ce_es_test_pc_precision, ce_es_test_pc_recall, ce_es_test_pc_f1, ce_es_test_cm, ce_es_test_predictions, ce_es_test_labels = evaluate_cross_encoder(\n",
    "    cross_encoder_retrained, cross_encoder_es_test_dataloader, cross_encoder_criterion, device\n",
    ")\n",
    "print_evaluation_results(\"Spanish XNLI Test (Cross-Encoder)\", ce_es_test_loss, ce_es_test_accuracy, ce_es_test_precision, ce_es_test_recall, ce_es_test_f1, ce_es_test_pc_precision, ce_es_test_pc_recall, ce_es_test_pc_f1, ce_es_test_cm)\n",
    "\n",
    "# Evaluate on German XNLI Test Set\n",
    "print(\"\\n--- Evaluating Retrained Cross-Encoder on German XNLI Test Set ---\")\n",
    "ce_de_test_loss, ce_de_test_accuracy, ce_de_test_precision, ce_de_test_recall, ce_de_test_f1, ce_de_test_pc_precision, ce_de_test_pc_recall, ce_de_test_pc_f1, ce_de_test_cm, ce_de_test_predictions, ce_de_test_labels = evaluate_cross_encoder(\n",
    "    cross_encoder_retrained, cross_encoder_de_test_dataloader, cross_encoder_criterion, device\n",
    ")\n",
    "print_evaluation_results(\"German XNLI Test (Cross-Encoder)\", ce_de_test_loss, ce_de_test_accuracy, ce_de_test_precision, ce_de_test_recall, ce_de_test_f1, ce_de_test_pc_precision, ce_de_test_pc_recall, ce_de_test_pc_f1, ce_de_test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63854c8-00ec-4a91-b7ef-b6c543a7d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 39: Collect and Organize Results\n",
    "print(\"\\n--- Collecting and Organizing Results ---\")\n",
    "\n",
    "# Organize Bi-Encoder results\n",
    "bi_encoder_results = {\n",
    "    \"MNLI Validation\": {\n",
    "        \"loss\": val_loss,\n",
    "        \"accuracy\": val_accuracy,\n",
    "        \"precision\": val_precision,\n",
    "        \"recall\": val_recall,\n",
    "        \"f1\": val_f1,\n",
    "        \"per_class_precision\": val_pc_precision,\n",
    "        \"per_class_recall\": val_pc_recall,\n",
    "        \"per_class_f1\": val_pc_f1,\n",
    "        \"confusion_matrix\": val_cm,\n",
    "    },\n",
    "    \"Spanish XNLI Test\": {\n",
    "        \"loss\": es_test_loss,\n",
    "        \"accuracy\": es_test_accuracy,\n",
    "        \"precision\": es_precision,\n",
    "        \"recall\": es_recall,\n",
    "        \"f1\": es_f1,\n",
    "        \"per_class_precision\": es_pc_precision,\n",
    "        \"per_class_recall\": es_pc_recall,\n",
    "        \"per_class_f1\": es_pc_f1,\n",
    "        \"confusion_matrix\": es_cm,\n",
    "    },\n",
    "    \"German XNLI Test\": {\n",
    "        \"loss\": de_test_loss,\n",
    "        \"accuracy\": de_test_accuracy,\n",
    "        \"precision\": de_precision,\n",
    "        \"recall\": de_recall,\n",
    "        \"f1\": de_f1,\n",
    "        \"per_class_precision\": de_pc_precision,\n",
    "        \"per_class_recall\": de_pc_recall,\n",
    "        \"per_class_f1\": de_pc_f1,\n",
    "        \"confusion_matrix\": de_cm,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Organize Cross-Encoder results\n",
    "cross_encoder_results = {\n",
    "    \"MNLI Validation\": {\n",
    "        \"loss\": ce_val_loss,\n",
    "        \"accuracy\": ce_val_accuracy,\n",
    "        \"precision\": ce_val_precision,\n",
    "        \"recall\": ce_val_recall,\n",
    "        \"f1\": ce_val_f1,\n",
    "        \"per_class_precision\": ce_val_pc_precision,\n",
    "        \"per_class_recall\": ce_val_pc_recall,\n",
    "        \"per_class_f1\": ce_val_pc_f1,\n",
    "        \"confusion_matrix\": ce_val_cm,\n",
    "    },\n",
    "    \"Spanish XNLI Test\": {\n",
    "        \"loss\": ce_es_test_loss,\n",
    "        \"accuracy\": ce_es_test_accuracy,\n",
    "        \"precision\": ce_es_test_precision,\n",
    "        \"recall\": ce_es_test_recall,\n",
    "        \"f1\": ce_es_test_f1,\n",
    "        \"per_class_precision\": ce_es_test_pc_precision,\n",
    "        \"per_class_recall\": ce_es_test_pc_recall,\n",
    "        \"per_class_f1\": ce_es_test_pc_f1,\n",
    "        \"confusion_matrix\": ce_es_test_cm,\n",
    "    },\n",
    "    \"German XNLI Test\": {\n",
    "        \"loss\": ce_de_test_loss,\n",
    "        \"accuracy\": ce_de_test_accuracy,\n",
    "        \"precision\": ce_de_test_precision,\n",
    "        \"recall\": ce_de_test_recall,\n",
    "        \"f1\": ce_de_test_f1,\n",
    "        \"per_class_precision\": ce_de_test_pc_precision,\n",
    "        \"per_class_recall\": ce_de_test_pc_recall,\n",
    "        \"per_class_f1\": ce_de_test_pc_f1,\n",
    "        \"confusion_matrix\": ce_de_test_cm,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Convert results to Pandas DataFrames for easier analysis\n",
    "bi_encoder_df = pd.DataFrame.from_dict(\n",
    "    {(dataset, metric): bi_encoder_results[dataset][metric]\n",
    "     for dataset in bi_encoder_results.keys()\n",
    "     for metric in bi_encoder_results[dataset].keys()},\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "cross_encoder_df = pd.DataFrame.from_dict(\n",
    "    {(dataset, metric): cross_encoder_results[dataset][metric]\n",
    "     for dataset in cross_encoder_results.keys()\n",
    "     for metric in cross_encoder_results[dataset].keys()},\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "# Display the DataFrames (will render as HTML tables in Jupyter)\n",
    "print(\"\\n--- Bi-Encoder Results DataFrame ---\")\n",
    "display(bi_encoder_df)\n",
    "\n",
    "print(\"\\n--- Cross-Encoder Results DataFrame ---\")\n",
    "display(cross_encoder_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e100f10-2b0c-47f1-afa2-7ea35dad4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 40: Analyze and Compare Performance\n",
    "# Assuming bi_encoder_results and cross_encoder_results dictionaries are populated\n",
    "\n",
    "def compare_models(bi_results, ce_results, dataset_name):\n",
    "    \"\"\"\n",
    "    Compares the performance of Bi-Encoder and Cross-Encoder models on a specific dataset.\n",
    "\n",
    "    Args:\n",
    "        bi_results (dict): Results dictionary for the Bi-Encoder model.\n",
    "        ce_results (dict): Results dictionary for the Cross-Encoder model.\n",
    "        dataset_name (str): Name of the dataset being compared (e.g., \"MNLI Validation\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performance Comparison: {dataset_name} ---\")\n",
    "    metrics_to_compare = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "    print(f\"{'Metric':<15} {'Bi-Encoder':<15} {'Cross-Encoder':<15} {'Difference':<15}\")\n",
    "    for metric in metrics_to_compare:\n",
    "        bi_val = bi_results[dataset_name][metric]\n",
    "        ce_val = ce_results[dataset_name][metric]\n",
    "        difference = ce_val - bi_val\n",
    "        print(f\"{metric:<15} {bi_val:<15.4f} {ce_val:<15.4f} {difference:<15.4f}\")\n",
    "\n",
    "    print(\"\\n--- Per-Class F1-Score Comparison ---\")\n",
    "    classes = [\"Entailment\", \"Neutral\", \"Contradiction\"]\n",
    "    print(f\"{'Class':<15} {'Bi-Encoder':<15} {'Cross-Encoder':<15} {'Difference':<15}\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        bi_f1 = bi_results[dataset_name][\"per_class_f1\"][i]\n",
    "        ce_f1 = ce_results[dataset_name][\"per_class_f1\"][i]\n",
    "        difference = ce_f1 - bi_f1\n",
    "        print(f\"{cls:<15} {bi_f1:<15.4f} {ce_f1:<15.4f} {difference:<15.4f}\")\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix (Cross-Encoder vs. Bi-Encoder) ---\")\n",
    "    print(\"\\nBi-Encoder Confusion Matrix:\")\n",
    "    print(bi_results[dataset_name][\"confusion_matrix\"])\n",
    "    print(\"\\nCross-Encoder Confusion Matrix:\")\n",
    "    print(ce_results[dataset_name][\"confusion_matrix\"])\n",
    "\n",
    "# Compare on MNLI Validation\n",
    "compare_models(bi_encoder_results, cross_encoder_results, \"MNLI Validation\")\n",
    "\n",
    "# Compare on Spanish XNLI Test\n",
    "compare_models(bi_encoder_results, cross_encoder_results, \"Spanish XNLI Test\")\n",
    "\n",
    "# Compare on German XNLI Test\n",
    "compare_models(bi_encoder_results, cross_encoder_results, \"German XNLI Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8762a67-ee3e-427d-ba7c-5ec2de7bfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 41: Statistical Significance Testing with More Iterations\n",
    "def perform_bootstrap_test(predictions_model1, predictions_model2, true_labels, metric=accuracy_score, n_iterations=10000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a paired bootstrap hypothesis test to compare the performance of two models.\n",
    "\n",
    "    Args:\n",
    "        predictions_model1 (np.array): Predictions of the first model.\n",
    "        predictions_model2 (np.array): Predictions of the second model.\n",
    "        true_labels (np.array): True labels.\n",
    "        metric (callable): Scoring function (e.g., accuracy_score).\n",
    "        n_iterations (int): Number of bootstrap iterations.\n",
    "        alpha (float): Significance level.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the observed difference, bootstrap differences,\n",
    "              p-value (two-tailed), and the confidence interval of the difference.\n",
    "    \"\"\"\n",
    "    n_samples = len(true_labels)\n",
    "    observed_difference = metric(true_labels, predictions_model2) - metric(true_labels, predictions_model1)\n",
    "    bootstrap_differences = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Resample with replacement\n",
    "        indices = resample(np.arange(n_samples), n_samples=n_samples, replace=True)\n",
    "        bootstrap_preds1 = predictions_model1[indices]\n",
    "        bootstrap_preds2 = predictions_model2[indices]\n",
    "        bootstrap_labels = true_labels[indices]\n",
    "\n",
    "        diff = metric(bootstrap_labels, bootstrap_preds2) - metric(bootstrap_labels, bootstrap_preds1)\n",
    "        bootstrap_differences.append(diff)\n",
    "\n",
    "    bootstrap_differences = np.array(bootstrap_differences)\n",
    "    p_value = np.mean(np.abs(bootstrap_differences) >= np.abs(observed_difference))\n",
    "    confidence_interval = np.percentile(bootstrap_differences, [(alpha / 2) * 100, (1 - alpha / 2) * 100])\n",
    "\n",
    "    return {\n",
    "        \"observed_difference\": observed_difference,\n",
    "        \"bootstrap_differences\": bootstrap_differences,\n",
    "        \"p_value\": p_value,\n",
    "        \"confidence_interval\": confidence_interval,\n",
    "    }\n",
    "\n",
    "# Perform bootstrap tests for accuracy on each dataset with increased iterations\n",
    "print(\"\\n--- Bootstrap Test (10000 iterations): Accuracy on MNLI Validation ---\")\n",
    "mnli_accuracy_results = perform_bootstrap_test(val_bi_encoder_predictions, ce_val_predictions, val_bi_encoder_labels)\n",
    "print(f\"Observed Accuracy Difference (Cross - Bi): {mnli_accuracy_results['observed_difference']:.4f}\")\n",
    "print(f\"P-value: {mnli_accuracy_results['p_value']:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{mnli_accuracy_results['confidence_interval'][0]:.4f}, {mnli_accuracy_results['confidence_interval'][1]:.4f}]\")\n",
    "\n",
    "print(\"\\n--- Bootstrap Test (10000 iterations): Accuracy on Spanish XNLI Test ---\")\n",
    "xnli_es_accuracy_results = perform_bootstrap_test(es_bi_encoder_predictions, ce_es_test_predictions, es_bi_encoder_labels)\n",
    "print(f\"Observed Accuracy Difference (Cross - Bi): {xnli_es_accuracy_results['observed_difference']:.4f}\")\n",
    "print(f\"P-value: {xnli_es_accuracy_results['p_value']:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{xnli_es_accuracy_results['confidence_interval'][0]:.4f}, {xnli_es_accuracy_results['confidence_interval'][1]:.4f}]\")\n",
    "\n",
    "print(\"\\n--- Bootstrap Test (10000 iterations): Accuracy on German XNLI Test ---\")\n",
    "xnli_de_accuracy_results = perform_bootstrap_test(de_bi_encoder_predictions, ce_de_test_predictions, de_bi_encoder_labels)\n",
    "print(f\"Observed Accuracy Difference (Cross - Bi): {xnli_de_accuracy_results['observed_difference']:.4f}\")\n",
    "print(f\"P-value: {xnli_de_accuracy_results['p_value']:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{xnli_de_accuracy_results['confidence_interval'][0]:.4f}, {xnli_de_accuracy_results['confidence_interval'][1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647beb73-4dba-495d-b69a-17ccf13e5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 42: Further Data Analysis and Visualization\n",
    "\n",
    "# Organize the results into a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Bi-Encoder', 'Cross-Encoder', 'Bi-Encoder', 'Cross-Encoder', 'Bi-Encoder', 'Cross-Encoder'],\n",
    "    'Dataset': ['MNLI Validation', 'MNLI Validation', 'Spanish XNLI Test', 'Spanish XNLI Test', 'German XNLI Test', 'German XNLI Test'],\n",
    "    'Accuracy': [val_accuracy, ce_val_accuracy, es_test_accuracy, ce_es_test_accuracy, de_test_accuracy, ce_de_test_accuracy],\n",
    "    'F1-Score (Macro)': [val_f1, ce_val_f1, es_f1, ce_es_test_f1, de_f1, ce_de_test_f1]\n",
    "})\n",
    "\n",
    "# Calculate percentage difference in Accuracy\n",
    "def calculate_percentage_difference(bi_val, cross_val):\n",
    "    return ((cross_val - bi_val) / bi_val) * 100\n",
    "\n",
    "mnli_acc_diff_percent = calculate_percentage_difference(val_accuracy, ce_val_accuracy)\n",
    "es_acc_diff_percent = calculate_percentage_difference(es_test_accuracy, ce_es_test_accuracy)\n",
    "de_acc_diff_percent = calculate_percentage_difference(de_test_accuracy, ce_de_test_accuracy)\n",
    "\n",
    "print(\"\\n--- Percentage Difference in Accuracy (Cross-Encoder vs. Bi-Encoder) ---\")\n",
    "print(f\"MNLI Validation: +{mnli_acc_diff_percent:.2f}%\")\n",
    "print(f\"Spanish XNLI Test: +{es_acc_diff_percent:.2f}%\")\n",
    "print(f\"German XNLI Test: +{de_acc_diff_percent:.2f}%\")\n",
    "\n",
    "# --- Visualizations ---\n",
    "\n",
    "# 1. Accuracy Comparison Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Dataset', y='Accuracy', hue='Model', data=results_df)\n",
    "plt.title('Accuracy Comparison: Bi-Encoder vs. Cross-Encoder')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylim(0.5, 0.9)  # Adjust y-axis limits for better visualization\n",
    "plt.show()\n",
    "\n",
    "# 2. F1-Score (Macro) Comparison Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Dataset', y='F1-Score (Macro)', hue='Model', data=results_df)\n",
    "plt.title('Macro F1-Score Comparison: Bi-Encoder vs. Cross-Encoder')\n",
    "plt.ylabel('Macro F1-Score')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylim(0.5, 0.9)  # Adjust y-axis limits for better visualization\n",
    "plt.show()\n",
    "\n",
    "# 3. Confusion Matrix Heatmaps (Optional - can be insightful for error patterns)\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(val_cm, [\"Entailment\", \"Neutral\", \"Contradiction\"], 'Bi-Encoder Confusion Matrix (MNLI Val)')\n",
    "plot_confusion_matrix(ce_val_cm, [\"Entailment\", \"Neutral\", \"Contradiction\"], 'Cross-Encoder Confusion Matrix (MNLI Val)')\n",
    "plot_confusion_matrix(es_cm, [\"Entailment\", \"Neutral\", \"Contradiction\"], 'Bi-Encoder Confusion Matrix (Spanish XNLI)')\n",
    "plot_confusion_matrix(ce_es_test_cm, [\"Entailment\", \"Neutral\", \"Contradiction\"], 'Cross-Encoder Confusion Matrix (Spanish XNLI)')\n",
    "plot_confusion_matrix(de_cm, [\"Entailment\", \"Neutral\", \"Contradiction\"], 'Bi-Encoder Confusion Matrix (German XNLI)')\n",
    "plot_confusion_matrix(ce_de_test_cm, [\"Entailment\", \"Neutral\", \"Contradiction\"], 'Cross-Encoder Confusion Matrix (German XNLI)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81658d7f-030e-4e80-a59d-a4dfdd218787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 43: Output Performance Metrics in Text Tables (Optional)\n",
    "def output_performance_metrics():\n",
    "    \"\"\"\n",
    "    Outputs performance metrics for Bi-Encoder and Cross-Encoder models in text tables.\n",
    "    This step is optional and can be called if detailed text-based comparisons are needed.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Performance Comparison: MNLI Validation ---\")\n",
    "    print(f\"{'Metric':<10}  {'Bi-Encoder':<15}  {'Cross-Encoder':<15}  {'Difference':<10}\")\n",
    "    print(f\"{'Accuracy':<10}  {val_accuracy:<15.4f}  {ce_val_accuracy:<15.4f}  {ce_val_accuracy - val_accuracy:<10.4f}\")\n",
    "    print(f\"{'Precision':<10}  {val_precision:<15.4f}  {ce_val_precision:<15.4f}  {ce_val_precision - val_precision:<10.4f}\")\n",
    "    print(f\"{'Recall':<10}     {val_recall:<15.4f}  {ce_val_recall:<15.4f}  {ce_val_recall - val_recall:<10.4f}\")\n",
    "    print(f\"{'F1':<10}         {val_f1:<15.4f}  {ce_val_f1:<15.4f}  {ce_val_f1 - val_f1:<10.4f}\")\n",
    "\n",
    "    print(\"\\n--- Per-Class F1-Score Comparison: MNLI Validation ---\")\n",
    "    print(f\"{'Class':<15}  {'Bi-Encoder':<15}  {'Cross-Encoder':<15}  {'Difference':<10}\")\n",
    "    for i, label in enumerate([\"Entailment\", \"Neutral\", \"Contradiction\"]):\n",
    "        print(f\"{label:<15}  {val_pc_f1[i]:<15.4f}  {ce_val_pc_f1[i]:<15.4f}  {ce_val_pc_f1[i] - val_pc_f1[i]:<10.4f}\")\n",
    "\n",
    "    print(\"\\n--- Performance Comparison: Spanish XNLI Test ---\")\n",
    "    print(f\"{'Metric':<10}  {'Bi-Encoder':<15}  {'Cross-Encoder':<15}  {'Difference':<10}\")\n",
    "    print(f\"{'Accuracy':<10}  {es_test_accuracy:<15.4f}  {ce_es_test_accuracy:<15.4f}  {ce_es_test_accuracy - es_test_accuracy:<10.4f}\")\n",
    "    print(f\"{'Precision':<10}  {es_precision:<15.4f}  {ce_es_test_precision:<15.4f}  {ce_es_test_precision - es_precision:<10.4f}\")\n",
    "    print(f\"{'Recall':<10}     {es_recall:<15.4f}  {ce_es_test_recall:<15.4f}  {ce_es_test_recall - es_recall:<10.4f}\")\n",
    "    print(f\"{'F1':<10}         {es_f1:<15.4f}  {ce_es_test_f1:<15.4f}  {ce_es_test_f1 - es_f1:<10.4f}\")\n",
    "\n",
    "    print(\"\\n--- Per-Class F1-Score Comparison: Spanish XNLI Test ---\")\n",
    "    print(f\"{'Class':<15}  {'Bi-Encoder':<15}  {'Cross-Encoder':<15}  {'Difference':<10}\")\n",
    "    for i, label in enumerate([\"Entailment\", \"Neutral\", \"Contradiction\"]):\n",
    "        print(f\"{label:<15}  {es_pc_f1[i]:<15.4f}  {ce_es_test_pc_f1[i]:<15.4f}  {ce_es_test_pc_f1[i] - es_pc_f1[i]:<10.4f}\")\n",
    "\n",
    "    print(\"\\n--- Performance Comparison: German XNLI Test ---\")\n",
    "    print(f\"{'Metric':<10}  {'Bi-Encoder':<15}  {'Cross-Encoder':<15}  {'Difference':<10}\")\n",
    "    print(f\"{'Accuracy':<10}  {de_test_accuracy:<15.4f}  {ce_de_test_accuracy:<15.4f}  {ce_de_test_accuracy - de_test_accuracy:<10.4f}\")\n",
    "    print(f\"{'Precision':<10}  {de_precision:<15.4f}  {ce_de_test_precision:<15.4f}  {ce_de_test_precision - de_precision:<10.4f}\")\n",
    "    print(f\"{'Recall':<10}     {de_recall:<15.4f}  {ce_de_test_recall:<15.4f}  {ce_de_test_recall - de_recall:<10.4f}\")\n",
    "    print(f\"{'F1':<10}         {de_f1:<15.4f}  {ce_de_test_f1:<15.4f}  {ce_de_test_f1 - de_f1:<10.4f}\")\n",
    "\n",
    "    print(\"\\n--- Per-Class F1-Score Comparison: German XNLI Test ---\")\n",
    "    print(f\"{'Class':<15}  {'Bi-Encoder':<15}  {'Cross-Encoder':<15}  {'Difference':<10}\")\n",
    "    for i, label in enumerate([\"Entailment\", \"Neutral\", \"Contradiction\"]):\n",
    "        print(f\"{label:<15}  {de_pc_f1[i]:<15.4f}  {ce_de_test_pc_f1[i]:<15.4f}  {ce_de_test_pc_f1[i] - de_pc_f1[i]:<10.4f}\")\n",
    "\n",
    "    print(\"\\n--- Statistical Significance (Accuracy) ---\")\n",
    "    print(f\"\\nMNLI Validation:\")\n",
    "    print(f\"  Observed Difference (Cross - Bi): {mnli_accuracy_results['observed_difference']:.4f}\")\n",
    "    print(f\"  P-value: {mnli_accuracy_results['p_value']:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: [{mnli_accuracy_results['confidence_interval'][0]:.4f}, {mnli_accuracy_results['confidence_interval'][1]:.4f}]\")\n",
    "\n",
    "    print(f\"\\nSpanish XNLI Test:\")\n",
    "    print(f\"  Observed Difference (Cross - Bi): {xnli_es_accuracy_results['observed_difference']:.4f}\")\n",
    "    print(f\"  P-value: {xnli_es_accuracy_results['p_value']:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: [{xnli_es_accuracy_results['confidence_interval'][0]:.4f}, {xnli_es_accuracy_results['confidence_interval'][1]:.4f}]\")\n",
    "\n",
    "    print(f\"\\nGerman XNLI Test:\")\n",
    "    print(f\"  Observed Difference (Cross - Bi): {xnli_de_accuracy_results['observed_difference']:.4f}\")\n",
    "    print(f\"  P-value: {xnli_de_accuracy_results['p_value']:.4f}\")\n",
    "    print(f\"  95% Confidence Interval: [{xnli_de_accuracy_results['confidence_interval'][0]:.4f}, {xnli_de_accuracy_results['confidence_interval'][1]:.4f}]\")\n",
    "\n",
    "    print(\"\\n--- Percentage Difference in Accuracy (Cross-Encoder vs. Bi-Encoder) ---\")\n",
    "    print(f\"MNLI Validation: +{mnli_acc_diff_percent:.2f}%\")\n",
    "    print(f\"Spanish XNLI Test: +{es_acc_diff_percent:.2f}%\")\n",
    "    print(f\"German XNLI Test: +{de_acc_diff_percent:.2f}%\")\n",
    "\n",
    "# Uncomment the following line to run this optional step\n",
    "# output_performance_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c3180-4f4a-41cd-98e1-29f28a67c1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
